{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxVcD7V0Wd8QmVAS6O9O5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naavveen/logistic_regression_anylasis/blob/main/logistic_regression_anylasis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2EWBP08O729"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as mp\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "2fwlIgz3QGy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('personality_dataset.csv')\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", data.shape)\n",
        "print(\"\\nDataset Info:\")\n",
        "print(data.info())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Define numeric and categorical columns\n",
        "numeric_columns = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']\n",
        "categorical_columns = ['Stage_fear', 'Drained_after_socializing']\n",
        "target_column = 'Personality'\n",
        "\n",
        "# Verify categorical values\n",
        "for col in categorical_columns:\n",
        "    print(f\"\\nUnique values in {col}:\")\n",
        "    print(data[col].value_counts(dropna=False))\n",
        "\n",
        "# Convert numeric columns to numeric, coercing errors\n",
        "for col in numeric_columns:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')"
      ],
      "metadata": {
        "id": "33w2mit1QPnf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[numeric_columns].corr()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IaPyQO4o83o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "for i ,col in enumerate(numeric_columns,1):\n",
        "    plt.subplot(3,2,i)\n",
        "    sns.boxplot(x=target_column,y=col,data=data)\n",
        "    plt.title(f'{col} by personality')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fsDj5_lxNGVX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data[numeric_columns + [target_column]], hue=target_column,diag_kind='hist')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GR1gqjXV9O3R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "data[target_column] = le.fit_transform(data[target_column])\n",
        "print(f\"Encoded classes: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
        "\n",
        "# Handle missing values\n",
        "# Numeric features: impute with median\n",
        "numeric_imputer = SimpleImputer(strategy='median')\n",
        "data[numeric_columns] = numeric_imputer.fit_transform(data[numeric_columns])\n",
        "\n",
        "# Categorical features: impute with mode\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "data[categorical_columns] = categorical_imputer.fit_transform(data[categorical_columns])\n",
        "\n",
        "# Encode categorical features\n",
        "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Update feature list\n",
        "encoded_columns = [col for col in data.columns if col != target_column]\n",
        "\n",
        "# Cap outliers instead of removing\n",
        "for col in numeric_columns:\n",
        "    Q1 = data[col].quantile(0.25)\n",
        "    Q3 = data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    data[col] = data[col].clip(lower=lower_bound, upper=upper_bound)"
      ],
      "metadata": {
        "id": "V_1a-KOG0e_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=data.drop(columns=target_column)\n",
        "y=data[target_column]\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(x,y,test_size=0.2,stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "EW27npgS1wlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aaa4450"
      },
      "source": [
        "# Re-run train/test split, SMOTE, feature engineering, and scaling\n",
        "# This is necessary because the data was modified in the previous step to handle numeric column issues.\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Handle class imbalance with SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, Y_train)\n",
        "\n",
        "# Ensure column names are preserved after SMOTE\n",
        "X_train = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
        "y_train = y_train_resampled\n",
        "\n",
        "# Impute missing values in original numeric columns after SMOTE\n",
        "numeric_columns = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']\n",
        "numeric_imputer = SimpleImputer(strategy='median')\n",
        "X_train[numeric_columns] = numeric_imputer.fit_transform(X_train[numeric_columns])\n",
        "X_test[numeric_columns] = numeric_imputer.transform(X_test[numeric_columns])\n",
        "\n",
        "# Feature Engineering (from cell 84zA-BUL35iu)\n",
        "X_train['Alone_to_Social_Ratio'] = X_train['Time_spent_Alone'] / (X_train['Social_event_attendance'] + 1)\n",
        "X_test['Alone_to_Social_Ratio'] = X_test['Time_spent_Alone'] / (X_test['Social_event_attendance'] + 1)\n",
        "X_train['Social_Comfort_Index'] = (X_train['Friends_circle_size'] + X_train['Post_frequency'] - X_train['Stage_fear_Yes']) / 3\n",
        "X_test['Social_Comfort_Index'] = (X_test['Friends_circle_size'] + X_test['Post_frequency'] - X_test['Stage_fear_Yes']) / 3\n",
        "X_train['Social_Overload'] = X_train['Drained_after_socializing_Yes'] * X_train['Social_event_attendance']\n",
        "X_test['Social_Overload'] = X_test['Drained_after_socializing_Yes'] * X_test['Social_event_attendance']\n",
        "\n",
        "# Impute missing values in engineered features before creating polynomial features\n",
        "engineered_features = ['Alone_to_Social_Ratio', 'Social_Comfort_Index', 'Social_Overload']\n",
        "for col in engineered_features:\n",
        "    for df in [X_train, X_test]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].replace([np.inf, -np.inf], np.nan) # Replace inf with NaN for imputation\n",
        "            imputer = SimpleImputer(strategy='median') # Use median for robustness to outliers\n",
        "            df[col] = imputer.fit_transform(df[[col]])\n",
        "\n",
        "# Polynomial features (from cell 84zA-BUL35iu)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "poly_features_train = poly.fit_transform(X_train[['Time_spent_Alone', 'Social_event_attendance', 'Friends_circle_size']])\n",
        "poly_features_test = poly.transform(X_test[['Time_spent_Alone', 'Social_event_attendance', 'Friends_circle_size']])\n",
        "poly_feature_names = poly.get_feature_names_out(['Time_spent_Alone', 'Social_event_attendance', 'Friends_circle_size'])\n",
        "\n",
        "# Remove original feature names from poly_feature_names\n",
        "original_features_poly = ['Time_spent_Alone', 'Social_event_attendance', 'Friends_circle_size']\n",
        "poly_feature_names = [name for name in poly_feature_names if name not in original_features_poly]\n",
        "\n",
        "# Add polynomial features to the DataFrames\n",
        "X_train[poly_feature_names] = poly_features_train[:, [poly.get_feature_names_out().tolist().index(name) for name in poly_feature_names]]\n",
        "X_test[poly_feature_names] = poly_features_test[:, [poly.get_feature_names_out().tolist().index(name) for name in poly_feature_names]]\n",
        "\n",
        "# Binning (from cell alWmnlaqqHWE)\n",
        "X_train['Time_spent_Alone_Binned'] = pd.qcut(X_train['Time_spent_Alone'], q=3, labels=['Low', 'Medium', 'High'])\n",
        "X_test['Time_spent_Alone_Binned'] = pd.qcut(X_test['Time_spent_Alone'], q=3, labels=['Low', 'Medium', 'High'])\n",
        "X_train = pd.get_dummies(X_train, columns=['Time_spent_Alone_Binned'], drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, columns=['Time_spent_Alone_Binned'], drop_first=True)\n",
        "\n",
        "# Apply scaling after feature engineering (from cell alWmnlaqqHWE)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Scaled X_train shape:\", X_train_scaled.shape)\n",
        "print(\"Scaled X_test shape:\", X_test_scaled.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train_resampled)"
      ],
      "metadata": {
        "id": "cyVQE2YqmX1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred =model.predict(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "LxARDTYFuQ9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(Y_test, y_pred))\n",
        "sensitivity = confusion_matrix(Y_test, y_pred)[0, 0] / (confusion_matrix(Y_test, y_pred)[0, 0] + confusion_matrix(Y_test, y_pred)[0, 1])\n",
        "print(f\"Sensitivity (True Positive Rate): {sensitivity}\")\n",
        "accuracy = np.mean(y_pred == Y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "TPxQ5pyNug4x",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}